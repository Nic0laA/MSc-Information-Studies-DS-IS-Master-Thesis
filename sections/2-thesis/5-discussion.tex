\section{Discussion}
\label{sec:discussion}
\todo[inline]{Edit discussion section}
% Compare your results with the state-of-the-art and reflect upon the results and limitations of the study. You can already hint at future work to which you come back in the conclusion section.

\subsection{Transformer models}

When searching for a competitor to the U-Net architecture, we considered transformer models due to their reported high performance over CNNs. From the limited work we conducted here, we found no convincing evidence that the transformer offers any performance benefits over the currently implemented U-Net model. The MTS transformer plateaus to a higher loss than the U-Net and ViT transformer models (Figure~\ref{fig:pretain_loss_curve}), indicating the model cannot extract complex features from the signals as well as the ViT transformer and U-Net CNN models. There are several possible reasons for this. Firstly, the MTS transformer is smaller than the ViT transformer, both in width and depth. The model hyperparameters were tuned based on the first 10 minutes of training since we were trying to optimise to training efficiency over a short period of time. However, this might not correspond to the best hyperparameter choice after several hours of training as the more complex features in the data are only learned later on in the training. The MTS model was also built to focus on sequential patterns in data, while the ViT model is better suited to extract visual features in images. The GW signal we were analysing consists of three main segments, and the parameters affect each of these segments differently. Therefore, it is important that the network is able to segment the image and treat each of the segments differently~\cite{bhardwaj2023peregrine}. This is a different task from a typical \enquote{time-series} analysis, where forecasting is often the main objective. Finally, in order to reduce the memory requirements, it was necessary to reduce the input dimensions of the data through patching, prior to feeding into the MTS model. This could also have negatively impacted the performance.

Transformer models are known to require large amounts of training data and long training times. Pre-training the model to learn general representations of the data, and then fine-tuning to increasingly tighter subsets of the data was attempted in order to reduce the computational time. However, the fine-tuning steps of the ViT still took longer than training the U-Net model from scratch.  This is likely because the ViT model was much larger and therefore took longer to train to new data.

%Due to the large number of trainable parameters (13M vs 0.7M), the fine-tuning of the ViT networks still took longer than training the U-Net from scratch. 

The ViT transformer and U-Net converged to the same loss value, therefore in terms of feature extraction they perform similarly. In general terms, the ViT appears to extract the extrinsic parameters better than U-Net, while U-Net extracts the intrinsic parameters better (Table~\ref{tab:loss_pretraining}). This could be because the extrinsic parameters affect the waveform on a more global level, while the intrinsic parameters affect the waveform more locally. The attention mechanism of the transformer might make it better at detecting the more global features.

\subsection{U-Net models}

Since we know the baseline U-Net model yields decent performance, we tried three variations of the network architecture and two variations of the \texttt{peregrine} run strategy to try improve upon the performance of the \texttt{peregrine} pipeline. After 8 rounds of simulations and training, we find the original U-Net architecture truncates the priors the most, indicating it is able to zoom-in to the parameter range of interest most efficiently (Table~\ref{tab:peregrine_network_results}). This is predominantly due to the reduction of luminosity distance ($d_L$) and spin angle ($\phi_{jl}$) priors. Both the train and test loss for the case with half the number of simulations, and 10\% pruned is higher in general, compared to the other cases. This indicates there was not enough training data, and the 10\% pruned model was overly simplistic so the more complex patterns in the data could not be learned in these cases.

% Cannot reproduce performance of Attention U-Net

Despite all the metrics given in Table~\ref{tab:peregrine_network_results}, it does not help us to predict how the posteriors will compare to the baseline. The only complete way to see this is to plot the posteriors and compare directly. The posteriors of the baseline case are considered to be the best possible result within statistical uncertainties, given the (noise) limitations inherently present in the data. The first posterior plot (Figure~\ref{fig:posterior_unet_half}) compares the \texttt{peregrine} run with half the number of simulations and the baseline \texttt{peregrine}. We can see that despite only half the number of training samples being used to train the network each round, the posteriors are still able to be resolved with the same precision. Small side-to-side shifts in the posteriors are expected as we have performed repeated inference over a signal with different statistical realisations~\cite{bhardwaj2023peregrine}. The fact that we can reproduce the posteriors with half the number of samples is beneficial because we can effectively half the network training and simulation time, and reduce disk storage space required to store the simulation data (287\,GB vs 175\,GB).

The posterior distributions of U-Net with no re-initialisation (Figure~\ref{fig:posterior_unet_noreinit})  reproduced all but three parameters ($a_1$, $a_2$ and $\phi_{jl}$) to the same level as the original U-Net. This can happen in later rounds of sequential TMNRE if some variables have not become highly constrained~\cite{Karchev_Trotta_Weniger_2023}. It is not clear exactly why that is the case, especially considering the network training has completed (evidenced by the loss and AUROC values).
\todo[inline]{Investigate this further}
In general, the posteriors of the Attention U-Net (Figure~\ref{fig:posterior_attention_unet}) were broader compared to the original (with the exception of $\psi$). The attention U-Net, and pruned models tend to resolve the $\psi$ parameter better than the original, but all other parameters are either the same or worse.
\todo[inline]{Investigate this further}

%We estimate that once the truncation fraction is below $\sim$5e-6, the parameter space is constrained enough for the marginal posteriors to be resolved.



\subsection{Limitations of research}

In this work so far, we have shown that changes to the U-Net architecture have not delivered any robust improvements to \texttt{peregrine}. The main improvement we identified was not related to the network architecture, but rather that similar results could be achieved even with half the amount of simulation data to train the network. This has been shown for only \textit{one} specific case, a waveform with low signal-to-noise ratio, and it is not necessarily generalizeable to all possible observed waveforms.

Another limitation is that we found it difficult to quantify the performance of the different networks using a single, or even multiple scalar metrics. We had the truncation fraction to signify how efficiently the network could identify the parameter region of interest, and the AUROC to assess the discriminative performance of the trained binary classifier. However, in the end, this could not tell us how the posterior distributions themselves were evolving through the rounds as we originally thought. It was therefore difficult to optimise the network in a straightforward way, as it was necessary to run several rounds of TMNRE before getting a clear indication on the overall performance of the network.

The changes we made to the network architecture were chosen based on current trends in the field while taking into consideration the time and resource constraints of the project. However, the possibilities for other options are endless, and likely there is something that works better than the U-Net, but we could just not identify it here with the limited things we try.

In terms of reproducibility, all runs of \texttt{peregrine} performed here can be repeated by running the code available on GitHub. The runs will need to be rerun from scratch since the simulation data is too large to be stored in GitHub. However, small differences in the results are expected due to the differing noise/statistical realisations of the simulated data. Therefore, the exact numbers will likely not be reproducible but the overall conclusions of each run should remain consistent with that stated here.



%The limitations should be reflected upon in terms such as reproducibility,  scalability,  generalizability,  reliability  and  validity. 

%* Difficulty to easily quantify performance
%* Could not try everything
%* Transformer Tuning based on 10 minutes.Representation learning.
%* Multiple rounds of data is difficult to optimise network for
%* statistical realisations of data may differ when runs are reporidced


%To compare the network performances we look at the number of epochs required per round, and the sampling fraction. We choose the sampling fraction because it is an indicator of how much the algorithm was able to truncate the priors, and is thus a proxy measurement of the precision capabilities of the network. Apart from the first round, we cannot easily compare loss functions 

%The overall objective of this work is to increase the efficiency of the \texttt{Peregrine} data analysis pipeline. The work will begin with reproducing the results from papers~\cite{bhardwaj2023peregrine} and~\cite{alvey2023things}, as this will form the benchmark to which the eventual results will be compared to.

%Many moving parts to problem - number of truncation rounds, the number of simulations per round, network architecture, the truncation, the sampling strategy of the priors.

%We will investigate whether some more active learning can be introduced to increase the efficiency of the sampling process. For instance, some parameters such as the chirp mass\footnote{The chirp mass is a combination of the two object masses in the binary system, and is a key factor in the gravitational wave frequency as the two objects spiral inwards toward each other.\\$\mathcal{M} = \frac{(m_1 \cdot m_2)^{3/5}}{(m_1 + m_2)^{1/5}}$} may be inferred early on with relatively high confidence, but currently in successive simulation rounds continues to be sampled from a uniform distribution within $\pm5\sigma$~\cite{Miller_TMNRE_2021}. We will investigate whether it's possible to be more selective in the sampling of parameters that are known with relatively high confidence in an un-biased way. Therefore, we can focus the simulation budget more on the parameters we know with less confidence. To do this in a systematic way, a complete survey of how influential each parameter is on the different segments of the GW signal will be carried-out first.

%Posteriors in earlier stages are much broader which demonstrates the need for multiple rounds and truncation of the priors.