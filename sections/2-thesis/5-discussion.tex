\section{Discussion}
\label{sec:discussion}
% Compare your results with the state-of-the-art and reflect upon the results and limitations of the study. You can already hint at future work to which you come back in the conclusion section.

\subsection{Transformer models}

We considered transformer models due to their reported high performance over CNNs. We found that the MTS transformer plateaus to a higher loss than the U-Net and ViT transformer models (Figure~\ref{fig:pretain_loss_curve}), indicating the model cannot extract complex features from the signals as well as the ViT transformer and U-Net CNN models. There are several possible reasons for this. Firstly, the MTS transformer is smaller than the ViT transformer, both in width and depth. The model hyperparameters were tuned based on the first 10 minutes of training since we were trying to optimise to training efficiency over a short period of time. However, this might have inadvertently led to an under-fitted model as the best hyperparameters for the first 10 minutes might not correspond to the best hyperparameters after several hours of training. More complex features in the data are only learned later on in the training. The MTS model was also built to focus on sequential patterns in data, while the ViT model is better suited to extract visual features in images. The GW signal we were analysing consists of three main segments, and the parameters affect each of these segments differently. Therefore, it is important that the network is able to segment the image and treat each of the segments differently~\cite{bhardwaj2023peregrine}. This is a different task from a typical \enquote{time series} analysis, where forecasting is often the main objective. Finally, in order to reduce the memory requirements, it was necessary to compress the input dimensions of the data through patching, prior to feeding into the MTS model. This could also have negatively impacted the performance.

Transformer models are known to require large amounts of training data and long training times. Pre-training the model to first learn general representations of the data, and then fine-tuning to increasingly tighter subsets of the data was attempted in order to reduce the overall computational cost. However, in the end, the fine-tuning steps of the ViT still took longer than training the U-Net model from scratch.  This is likely because the ViT model was much larger and therefore took longer to train to new data.

%Due to the large number of trainable parameters (13M vs 0.7M), the fine-tuning of the ViT networks still took longer than training the U-Net from scratch. 

The ViT transformer and U-Net converged to the same loss value, therefore in terms of feature extraction they perform similarly. In general terms, the ViT seems to extract the extrinsic parameters better than U-Net, while U-Net extracts the intrinsic parameters better (Table~\ref{tab:loss_pretraining}). This could be because the extrinsic parameters affect the waveform on a more global level, while the intrinsic parameters affect the waveform more locally. The self-attention mechanism of the transformer might make it better at detecting the more global features. To conclude, we found no convincing evidence that the transformer models offer any performance benefits over the currently implemented U-Net model.

\subsection{U-Net models}

After 8 rounds of simulations and network training, the original U-Net architecture truncates the priors the most, indicating it is able to zoom-in to the parameter range of interest most efficiently (Table~\ref{tab:peregrine_network_results}). This is predominantly due to the range reduction in the luminosity distance ($d_L$) and spin angle ($\phi_{jl}$) parameters. In general, both the train and test loss for the case with half the number of simulations, and 10\% pruned is higher, compared to the other cases. This suggests there was not enough training data in the case of half the number of simulations, and the model became overly simplistic in the case of the 10\% pruned, so the more complex patterns in the data could not be learned.

% Since we know the baseline U-Net model yields decent performance, we tried three variations of the network architecture and two variations of the \texttt{peregrine} run strategy to try improve upon the performance of the \texttt{peregrine} pipeline.
% Cannot reproduce performance of Attention U-Net
% U-Net half early stopping, higher loss, reaches overfitting faster. But still can generalise to out of sample data good enough.
% Network training is bottleneck, but less samples trains faster
% Too many parameters

Despite all the metrics given in Table~\ref{tab:peregrine_network_results}, it does not really give us a clear indication on how the posteriors will compare to the baseline. The best way is to compare qualitatively/visually with plots. The posteriors of the baseline case are considered to be the best possible result within statistical uncertainties, given the (noise) limitations inherently present in the data. The first posterior plot (Figure~\ref{fig:posterior_unet_half}) compares the \texttt{peregrine} run with half the number of simulations and the baseline \texttt{peregrine}. We can see that despite only half the number of training samples being used to train the network each round, the posteriors are still able to be resolved with the same precision. Small side-to-side shifts in the posteriors are expected as we have performed repeated inference over a signal with different statistical realisations~\cite{bhardwaj2023peregrine}. The fact that we can reproduce the posteriors with half the number of samples is beneficial because we can reduce both the network training time and disk storage space required to store the simulation data (175\,GB vs 287\,GB). The reason the network training time is reduced is due to the early stopping condition which terminates training if the validation loss has not decreased within 7 epochs. It takes half the time to cycle through an epoch, and with fewer samples the model begins to over-fit to the training data faster which triggers the early stopping. Despite this, also evidenced with the higher test loss values, it is still sufficient to construct the posteriors with high precision.

The posterior distributions of U-Net with no re-initialisation (Figure~\ref{fig:posterior_unet_noreinit})  reproduced all but three parameters ($a_1$, $a_2$ and $\phi_{jl}$) to the same level as the original U-Net. This can happen in later rounds of sequential TMNRE if some variables have not become highly constrained~\cite{Karchev_Trotta_Weniger_2023}. It is not clear exactly why that is the case, especially considering the network training has completed, as evidenced by the loss and AUROC values. One possible explanation is that part of the network got stuck in a local minimum between rounds. %\todo{Investigate}
In general, the posteriors of the Attention U-Net (Figure~\ref{fig:posterior_attention_unet}) were broader compared to the original (with the exception of $\psi$). Although the 8 TMNRE rounds completed training $\sim$8\% faster than the baseline, the overall performance gains of Attention U-Net could not be reproduced here~\cite{Oktay_2018_AUNet}.

The pruned models 5\% and 10\% give somewhat inconsistent results. The 10\% pruned model has reproduced the posteriors better than the 5\%, while the loss and truncation factor is higher and AUROC is lower. Many of the posteriors of the 5\% pruned have shifted relative to the ground-truth values. This highlights the difficulty of using the network performance metrics to predict the outcome of the posteriors for all 15 parameters.

% The attention U-Net, and pruned models tend to resolve the $\psi$ parameter better than the original, but all other parameters are either the same or worse.
% \todo[inline]{Investigate this further}
%We estimate that once the truncation fraction is below $\sim$5e-6, the parameter space is constrained enough for the marginal posteriors to be resolved.

\subsection{Limitations of research}

The modifications to the U-Net architecture that we assessed in this work have not delivered convincing robust improvements to \texttt{peregrine}. The main improvement we identified was not related to the network architecture, but rather that similar results could be achieved even with half the amount of simulation data to train the network. It is however important to note that this has been shown for only \textit{one} specific case, an example waveform with low signal-to-noise ratio, and it is not necessarily generalizable to all possible observed waveforms. To improve the reliability of this outcome, it should be confirmed with additional target waveforms.

Another limitation is the difficulty we found to quantify the performance of the different networks using a single, or even multiple scalar metrics and then extrapolate this to the posterior distributions. We had the truncation fraction to signify how efficiently the network could identify the parameter region of interest, and the AUROC to assess the discriminative performance of the trained binary classifier. However, we identified validity issues with this because it could not tell us how the posterior distributions themselves were evolving through the rounds. It was therefore difficult to optimise the network in a straightforward way (using a single objective function/value), since it was necessary to run several rounds of simulation-training-inference-truncation before a clear indication on the performance of \texttt{peregrine} could be revealed.

The changes we made to the network architecture were selected based on current state-of-the-art trends in the field while taking into consideration the time and resource constraints of the project. From endless possibilities, it was possible to only try a limited number of things. In terms of reproducibility, all run scripts and results of the \texttt{peregrine} runs are available on GitHub. The simulation data however will need to be created from scratch since it is too large to be stored in GitHub. Small differences in the results will be expected due to the differing noise/statistical realisations of the simulated data. Therefore, the exact numbers will likely not be replicated but the overall outcome of each case should be reproducible.

%The limitations should be reflected upon in terms such as reproducibility,  scalability,  generalizability,  reliability  and  validity. 

%* Difficulty to easily quantify performance
%* Could not try everything
%* Transformer Tuning based on 10 minutes.Representation learning.
%* Multiple rounds of data is difficult to optimise network for
%* statistical realisations of data may differ when runs are reporidced

%To compare the network performances we look at the number of epochs required per round, and the sampling fraction. We choose the sampling fraction because it is an indicator of how much the algorithm was able to truncate the priors, and is thus a proxy measurement of the precision capabilities of the network. Apart from the first round, we cannot easily compare loss functions 

%The overall objective of this work is to increase the efficiency of the \texttt{Peregrine} data analysis pipeline. The work will begin with reproducing the results from papers~\cite{bhardwaj2023peregrine} and~\cite{alvey2023things}, as this will form the benchmark to which the eventual results will be compared to.

%Posteriors in earlier stages are much broader which demonstrates the need for multiple rounds and truncation of the priors.

