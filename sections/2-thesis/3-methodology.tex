\section{Methodology}
\label{sec:methodology}
% Focus on what you add to the existing method. Explain what you will do and why (and how). Do not forget to characterize your research design. There should be an evaluation plan in this section. (For DS students, this normally means using manually labelled or ground truth data.)

\todo[inline]{Very much still a work in progress}

% ============================================================================ %
% Description of the data
% ============================================================================ %
\subsection{Description of the data}

The waveform data consists of 1D signals in both time and frequency domains, collected from three separate detectors (referred to as H1, L1, V1) that have captured the event simultaneously. The signal in the time domain consists of three channels (three detectors) and 8192 data points corresponding to 4\,s of collection time at a sampling frequency of 2048\,Hz. An example of the signal in the time domain is shown in Figure~\ref{fig:obs_time_domain}. The time domain can be transformed to the frequency domain through the Fourier transform. The frequency domain contains 4197 data points and six channels (real and imaginary parts of the time signal from three detectors). An example of the signal in the frequency domain is shown in Figure~\ref{fig:obs_freq_domain}. Both the time and frequency domains are fed into the neural network, since they have different information about the GW encoded~\cite{bhardwaj2023peregrine}. 

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{media/images/obs_time_domain_lowSNR.png}
  \caption{Example of generated gravitational wave signal in the time domain. For clarity, signals from three detectors are shown without noise. The noise signal shows the H1 signal with noise added, which is the actual signal used to train the network. The two black holes merge at the moment t=0s. }
  \label{fig:obs_time_domain}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{media/images/obs_freq_domain_lowSNR.png}
  \caption{Example of generated gravitational wave signal in the frequency domain. For clarity, signals from three detectors are shown without noise. The noise signal shows the H1 signal with noise added, which is the actual signal used to train the network. The two black holes merge at the moment t=0s. }
  \label{fig:obs_freq_domain}
\end{figure}

% ============================================================================ %
% Generation of the waveforms
% ============================================================================ %

\subsection{Generation of the waveforms}

All of the waveforms used in this study were generated using the open-source Bilby code~\cite{Ashton_Bilby_2019}. The Bilby code has functionality to 

which was controlled using the \texttt{swyft} library~\cite{Miller2022}, which is highly efficient and allows the user to implement their own \texttt{Simulator} class~\cite{bhardwaj2023peregrine}.



The waveform data consists of 1D signals 

Gravitational waves are detected using highly sensitive laser interferometers. 

 The gravitational waveforms of three LIGO detectors (Hanford, Washington and Livingston) were generated as a function of 15 independent parameters. Ten of these parameters represented intrinsic properties of the source e.g. the masses and spins of the two black holes, while five of these are extrinsic parameters e.g. the distance from the source and orientation in the sky with respect to the observations. Further details of the parameters are listed in Table~\ref{tab:gw_parameters}. Simulating the waveforms for training the neural network is necessary since we need $\sim$100000 waveforms for training and experimentally measured signals are rare (only 90 detections so far).

Only low SNR case considered because high SNR is unrealistic and it is important for the network to determine signal from noise. Peregrine benchmark.

\todo[inline]{Complete table}

\begin{table}[htb]
\centering
\caption{Description and type of the parameters that fully describe the detected gravitational waves. The injection values refer to the parameters used for the generated target observation, $x_0$.}
\label{tab:gw_parameters}
\hspace*{-0.5cm}
\begin{tabular}{|l|l|l|} 
\hline
\textbf{Parameter} & \textbf{Prior dist.} & \textbf{Injection Value} \\ 
\hline
\textit{Intrinsic} & & \\ 
Mass ratio, \( q \) & \( U(0.125, 1) \) & 0.8858 \\ 
Chirp mass, \( M \) [\( M_{\odot} \)] & \( U(25, 100) \) & 32.14 \\ 
Inclination angle, \( \theta_{jn} \) [rad] & sin(0, \( \pi \)) & 0.4432 \\ 
Phase, \( \phi_c \) [rad] & \( U(0, 2\pi) \) & 5.089 \\ 
Tilt angle, \( \theta_1 \) [rad] & sin(0, \( \pi \)) & 1.497 \\ 
Tilt angle, \( \theta_2 \) [rad] & sin(0, \( \pi \)) & 1.102 \\ 
Spin, \( a_1 \) & \( U(0.05, 1) \) & 0.9702 \\ 
Spin, \( a_2 \) & \( U(0.05, 1) \) & 0.8118 \\ 
Spin angle, \( \phi_{12} \) [rad] & \( U(0, 2\pi) \) & 6.220 \\ 
Spin angle, \( \phi_{jl} \) [rad] & \( U(0, 2\pi) \) & 1.885 \\ 
\hline
\textit{Extrinsic} & & \\ 
Luminosity Distance, \( d_L \) [Mpc] & \( U_{\text{vol}}(100, 2000) \) & 900 \\ 
Right ascension, \( \alpha \) [rad] & \( U(0, 2\pi) \) & 5.556 \\ 
Declination, \( \delta \) [rad] & cos(-\( \pi/2 \), \( \pi/2 \)) & 0.071 \\ 
Polarisation angle, \( \psi \) [rad] & \( U(0, \pi) \) & 1.100 \\ 
Merger time, \( t_c \) [GPS s] & \( U(-0.1, 0.1) \) & 0.000 \\ 
\hline
\end{tabular}
\end{table}


The data used in this study was simulated using the open-source Bilby code~\cite{Ashton_Bilby_2019}.
Only low signal to noise signal will be considered here since the high SNR is unphysical and the low SNR is more challenging. Many moving parts to problem - number of truncation rounds, the number of simulations per round, network architecture, the truncation, the sampling strategy of the priors.

% ============================================================================ %
% Peregrine
% ============================================================================ %
\subsection{Peregrine inference pipeline}

The overall objective of this work is to increase the efficiency of the \texttt{Peregrine} data analysis pipeline. The work will begin with reproducing the results from papers~\cite{bhardwaj2023peregrine} and~\cite{alvey2023things}, as this will form the benchmark to which the eventual results will be compared to.

The workflow for the simulation-based inference technique for the analysis of the gravitational wave signals as implemented in \texttt{peregrine}~\cite{bhardwaj2023peregrine} is shown in Figure~\ref{fig:peregrine_pipeline}. The process starts by setting the 15 parameters of the `target observation' and then generating the example waveform to be analysed. This is done so there are `ground-truth' parameter values that you can compare your final posterior probability density distributions with and validate the overall method. If we use a true experimentally measured signal, then we can never know for certain what the `ground-truth' values of the parameters are. Given the accuracy that we can forward model the GW signals with, once the method is validated with the simulated waveforms, it is expected to work equally well with true experimental measurements.

\begin{figure}[htb]
    \centering
    \begin{tikzpicture}[node distance=2cm]
        \node (sampling) [sample] {\textbf{Sample} parameters $\boldsymbol{\theta}_{GW}$ from prior $p(\boldsymbol{\theta}_{GW})$};
        \node (fsimulator) [simulator, below of=sampling] {\textbf{Simulate} data $\boldsymbol{x}(\theta_{GW}) = h(\theta_{GW}) + n_{IFO}$};
        \node (network) [network, below of=fsimulator, yshift=-0.25cm] {\textbf{Train} network to estimate likelihood-to-evidence ratios $r(\boldsymbol{x};\theta_k) = p(\theta_k|\boldsymbol{x})/p(\theta_k)$ for all parameters $k$};
        \node (inference) [inference, below of=network] {Bayesian \textbf{Inference}};
        \node (prior) [pinput, left of=inference, xshift=-1cm] {Prior sample from $p(\theta)$};
        \node (target) [tinput, right of=inference, xshift=1cm] {Target Observation $\boldsymbol{x}_0$};
        \node (ratios) [output, below of=inference, xshift=-1.5cm] {Ratios $r(\boldsymbol{x}_0;\theta)$};
        \node (posterior) [output, below of=inference, xshift=1.5cm] {Posteriors $p(\theta|\boldsymbol{x}_0)$};
        \draw [arrow] (sampling) -- node[anchor=west] {$\boldsymbol{\theta}_{GW}$} (fsimulator);
        \draw [arrow] (fsimulator) -- node[anchor=west] {$\boldsymbol{\theta}_{GW},\boldsymbol{x}$} (network);
        \draw [arrow] (network) -- (inference);
        \draw [arrow] (prior) -- (inference);
        \draw [arrow] (target) -- (inference);
        \draw [arrow] (inference) -- (posterior);
        \draw [arrow] (inference) -- (ratios);
        \draw [arrow] (ratios) -- +(-3,0) |- node[anchor=west, yshift=-2cm, text width=2cm]{Using $r(\boldsymbol{x}_0;\theta)$ update prior $p(\boldsymbol{\theta}_{GW})$ and repeat rounds until converged} (sampling);
    \end{tikzpicture}
    \caption{High-level overview of the simulation-based inference method used for this work.}
    \label{fig:peregrine_pipeline}
\end{figure}

\begin{figure}[htb]
    \centering
    \begin{tikzpicture}[node distance=2cm]
			%%% NODES
			\node (dt) [per_input, label=west:\small 8192$\times$3] { $d(t)+n(t)$ };
			\node (df) [per_input, right of=dt, xshift=1cm, label=east:\small 4097$\times$6] {$\tilde{d}(f)+\tilde{n}(f)$};
			\node (unet_t) [per_unet, below of=dt, yshift=1cm, text width=2cm] { U-Net / Transformer };
			\node (unet_f) [per_unet, below of=df, yshift=1cm, text width=2cm] { U-Net / Transformer };
			\node (feature_t) [per_features, below of=unet_t, label=west:\small 16$\times$1, yshift=1cm] { $t$ feature map };	
			\node (feature_f) [per_features, below of=unet_f, label=east:\small 16$\times$1, yshift=1cm] { $f$ feature map };
			\node (concat) [per_concat, below of=feature_t, xshift=1.5cm, label=east:\small 32$\times$1, yshift=1cm] { Concatenated };
			\node (MLP) [per_MLP, below of=concat, yshift=1cm] { MLP (swyft) };
			\node (labels) [per_labels, right of=MLP, xshift=0.5cm, text width=1.0cm] { $\theta_{GW}$ labels };
			\node (lograts) [per_logratios, below of=MLP, label=east:\small 15$\times$1, yshift=1cm] { logratios };
			%%% EDGES			
			\draw [arrow_unet] (dt) -- node[anchor=west]{}(unet_t);
			\draw [arrow_unet] (df) -- node[anchor=west]{}(unet_f);
			\draw [arrow_unet] (unet_t) -- node[anchor=west]{}(feature_t);
			\draw [arrow_unet] (unet_f) -- node[anchor=west]{}(feature_f);
			\draw [arrow_unet] (feature_t) -- node[anchor=west]{}(concat);
			\draw [arrow_unet] (feature_f) -- node[anchor=west]{}(concat);
			\draw [arrow_unet] (concat) -- node[anchor=west]{}(MLP);
			\draw [arrow_unet] (labels) -- node[anchor=west]{}(MLP);
			\draw [arrow_unet] (MLP) -- node[anchor=west]{}(lograts);
    \end{tikzpicture}
    \caption{Details of the neural network step in Peregrine. The numbers outside the boxes represent the output size of each step. Details of the U-Net architecture are given in Figure \ref{fig:Attention_UNet_arch} in the appendix.}
    \label{fig:peregrine_network_step}
\end{figure}


Fifteen individual binary classifiers. Minimise binary cross-entropy loss.

Noise shuffling.

Number of trainable parameters for each model.

% \subsection{Optimising of sampling efficiency}

% We will investigate whether some more active learning can be introduced to increase the efficiency of the sampling process. For instance, some parameters such as the chirp mass\footnote{The chirp mass is a combination of the two object masses in the binary system, and is a key factor in the gravitational wave frequency as the two objects spiral inwards toward each other.\\$\mathcal{M} = \frac{(m_1 \cdot m_2)^{3/5}}{(m_1 + m_2)^{1/5}}$} may be inferred early on with relatively high confidence, but currently in successive simulation rounds continues to be sampled from a uniform distribution within $\pm5\sigma$~\cite{Miller_TMNRE_2021}. We will investigate whether it's possible to be more selective in the sampling of parameters that are known with relatively high confidence in an un-biased way. Therefore, we can focus the simulation budget more on the parameters we know with less confidence. To do this in a systematic way, a complete survey of how influential each parameter is on the different segments of the GW signal will be carried-out first.

The current rendition of the Peregrine pipeline requires 8 rounds of sequential TMNRE, 720000 simulated waveforms and around 12.5 hours (on a single A100 gpu with 18 cpu cores) to completely reconstruct the posteriors of the fifteen parameters. Of these 12.5 hours, around 10 is for training the network and 2.3 is for generating the waveforms used for training the network. Our primary focus is therefore the optimisation of the pipeline on the network itself, and as a secondary the number of simulated waveforms.

Trainer settings.

Joint and marginal pairs.

% ============================================================================ %
% Overview
% ============================================================================ %
\subsection{Overview of approach}

We ran Peregrine with the following changes to the network architecture. For maximum consistency, all other settings remained the same. 

% ============================================================================ %
% Attention U-Net
% ============================================================================ %
\subsection{Attention U-Net}

The UNet modelled was modified to include the attention gate.

% ============================================================================ %
% Transformer models
% ============================================================================ %
\subsection{Transformer models}

Two transformer architectures were considered, the so-called 1D vanilla vision transformer~\cite{Dosovitskiy_2021_ViT,}, and an architecture that was specifically designed for multi-variate time series representation learning~\cite{Zerveas_2020_mvts}.

\subsubsection{Hyperparameter tuning}

To find the optimal hyperparameters for each of the two models, we used the Python library Ray Tune~cite{liaw2018tune} in combination with the Hyperopt \enquote{Tree-structured Parzen Estimator} search algorithm~\cite{Bergstra_Bardenet_Bengio_KÃ©gl_2011}. Ray Tune provides the framework to explore the hyperparameter space in a highly efficient way. 

Ray Tune works by at first randomly selecting hyperparameters (in this case )

\begin{table}[h]
    \centering
    \caption{Hyperparameters for ViT}
    \label{tab:config_parameters}
    \begin{tabular}{@{}ll@{}}
    \toprule
    Parameter       & Values \\ 
    \midrule
    batch size     & 16, 32, 64, 128 \\
    learning rate  & loguniform(3e-5, 2e-4) \\
    patch size     & 4, 8, 16 \\
    num classes    & 16, 24, 32 \\
    dim             & 256, 512, 1024 \\
    depth           & 4--10 \\
    heads           & 4--10 \\
    mlp dim        & 1024, 2048 \\
    dropout         & 0, 0.05, 0.1 \\
    emb dropout    & 0, 0.05, 0.1 \\
    max num epochs & 10  \\ 
    \bottomrule
    \end{tabular}
\end{table}


\begin{table}[h]
    \centering
    \caption{Hyperparameter options for tuning in mvts}
    \label{tab:config_parameters}
    \begin{tabular}{@{}ll@{}}
    \toprule
    Parameter       & Values \\ 
    \midrule
    batch size & 16,32,64 \\
    learning rate & 1e-3, 3e-4, 1e-4 \\
    dim model & 128, 256, 512 \\
    num heads & 4, 8, 16 \\
    num layers & 3, 4, 5, 6, 7, 8 \\
    dim feedforward & 256, 512, 1024 \\
    dropout & 0, 0.05, 0.1 \\
    pos encoding & fixed, learnable \\
    max num epochs & 10 \\
    \end{tabular}
\end{table}

\subsubsection{Pretraining}

Transformers were pretrained for 24 hours with single A100 gpu on 2 million waveforms. After pretraining the saved weights were used to initialise the network during each Peregrine run. 95\% train, 5\% test. Regular validation checks of loss to ensure no overfitting and measure learning performance.


% ============================================================================ %
% Pruning
% ============================================================================ %
\subsection{Pruning}

To reduce the size of the network we implemented some pruning methods with the DepGraph library~\cite{Fang_Ma_Song_Mi_Wang_2023}.

%The architecture of the network currently implemented in \texttt{peregrine} is the U-Net architecture~\cite{Ronneberger_UNet_2015}. Given the advances in machine learning and CNN architectures since 2015, it is believed that this network can be improved upon. The optimisation of this network architecture will be the main focus of this thesis. 

%Investigative studies will be performed to find the best network architectures most suitable for the data format. We will first try with LSTM's since they are known to work well with noisy time-series data. The chosen network architecture also needs to be capable of segmenting the signal into three components, representing the different stages of the merger event -- inspiral, merger and ringdown~\cite{Pan_GW_2014}. Each of the parameters in $\boldsymbol{\theta}_{GW}$ are impacted differently in the different phases~\cite{bhardwaj2023peregrine}. Given the network is a binary classifier that classifies between joint and marginally drawn sample pairs, we will assess the performance of the network with the ROC curve.


% ============================================================================ %
% Evaluation
% ============================================================================ %
\subsection{Evaluation}

%The changes to the \texttt{peregrine} analysis pipeline will be fully benchmarked against the original \texttt{peregrine}, both in terms of accuracy of final result and required runtime. The results of the original \texttt{peregrine} have themselves been benchmarked against established likelihood-based methods~\cite{Speagle_2020}, and found to be in good agreement. Therefore, in this work we think it is sufficient to compare only with the original \texttt{peregrine}. To demonstrate the applicability of the method to real gravitational wave measurements, if time permits, we will also test the approach using real experimental data.

